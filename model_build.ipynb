{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/movie-reviews-tf/venv/local/lib/python2.7/site-packages/sklearn/utils/random.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._random import sample_without_replacement\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import io\n",
    "import re\n",
    "import glob\n",
    "import tempfile\n",
    "import math\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata, dataset_schema\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import tfrecordio\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from djr_py.tf.dataset import fetch_tf_records\n",
    "from djr_py.tf.util import get_available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil cp gs://djr-data/movie-reviews/aclImdb_v1.tar.gz .\n",
    "#!tar -xzf aclImdb_v1.tar.gz && rm aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil cp gs://djr-data/movie-reviews/glove.twitter.27B.zip .\n",
    "#!unzip glove.twitter.27B.zip -d glove && rm glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: RE2 does not support constructs for which only backtracking solutions are known to exist. Thus, backreferences and look-around assertions are not supported!  As a result, I can't put this in tf.regex_replace(...), and it must live outside the input serving function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this pulls out our proper nouns and treats them as single words\n",
    "def proper_preprocessing(review):\n",
    "    proper = r\"([A-Z]([a-z]+|\\.)(?:\\s+[A-Z]([a-z]+|\\.))*(?:\\s+[a-z][a-z\\-]+){0,2}\\s+[A-Z]([a-z]+|\\.)(?:\\s+([0-9]+(?:,[0-9]+)?))?)\"\n",
    "    space_between_brackets = r\"[\\.\\s]+(?=[^\\[\\]]*]])\"\n",
    "    brackets = r\"(?:[\\[]{2})(.*?)(?:[\\]]{2})\"\n",
    "    \n",
    "    review = re.sub(proper, '[[\\\\1]]', review)\n",
    "    review = re.sub(space_between_brackets, '~', review)\n",
    "    review = re.sub(brackets, '\\\\1', review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(g, out):\n",
    "    inputs = glob.glob(g)\n",
    "    np.random.shuffle(inputs)\n",
    "    with tf.python_io.TFRecordWriter(out) as writer:\n",
    "        for i in inputs:\n",
    "            label = 1 if i.split('/')[2] == 'pos' else 0\n",
    "            with open(i, 'r') as f:\n",
    "                review = f.read()\n",
    "            \n",
    "            example = tf.train.Example()\n",
    "            example.features.feature['review'].bytes_list.value.append(proper_preprocessing(review))\n",
    "            example.features.feature['label'].int64_list.value.append(label)\n",
    "                                \n",
    "            writer.write(example.SerializeToString())\n",
    "    \n",
    "load_data('aclImdb/train/[posneg]*/*.txt', 'data/train.tfrecord')\n",
    "load_data('aclImdb/test/[posneg]*/*.txt', 'data/test.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TFT to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for raw data\n",
    "RAW_DATA_FEATURE = {\n",
    "    'review': tf.FixedLenFeature(shape=[1], dtype=tf.string),\n",
    "    'label': tf.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec(RAW_DATA_FEATURE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf tft_output/transform_fn \n",
    "!rm -Rf tft_output/transformed_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:root:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "# train our tft transformer\n",
    "with beam.Pipeline() as pipeline:\n",
    "    with beam_impl.Context(temp_dir=tempfile.mkdtemp()):\n",
    "        coder = tft.coders.ExampleProtoCoder(RAW_DATA_METADATA.schema)\n",
    "\n",
    "        train_data = (\n",
    "            pipeline\n",
    "            | 'ReadTrain' >> tfrecordio.ReadFromTFRecord('data/train.tfrecord')\n",
    "            | 'DecodeTrain' >> beam.Map(coder.decode))\n",
    "\n",
    "        test_data = (\n",
    "            pipeline\n",
    "            | 'ReadTest' >> tfrecordio.ReadFromTFRecord('data/test.tfrecord')\n",
    "            | 'DecodeTest' >> beam.Map(coder.decode))\n",
    "\n",
    "        \n",
    "        # remove links, tags, quotes, apostraphes\n",
    "        # bracketize proper nouns, names, and numbers\n",
    "        # then lowercase, split by punctuation, and remove low frequency words\n",
    "        def preprocessing_fn(inputs):\n",
    "            remove = '|'.join([\"https?:\\/\\/(www\\.)?([^\\s]*)\", \"<([^>]+)>\", \"\\'\", \"\\\"\"])\n",
    "            punctuation = r\"([.,;!?\\(\\)\\/])+\"\n",
    "            \n",
    "            reviews = tf.reshape(inputs['review'], [-1])\n",
    "            \n",
    "            reviews = tf.regex_replace(reviews, remove, '')\n",
    "            reviews = tf.regex_replace(tf.regex_replace(reviews, punctuation, ' \\\\1 '), r\"\\s+\", ' ')\n",
    "            \n",
    "            for letter in list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
    "                reviews = tf.regex_replace(reviews, letter, letter.lower())\n",
    "                \n",
    "            terms = tf.string_split(reviews, ' ')\n",
    "            terms_indices = tft.compute_and_apply_vocabulary(terms, frequency_threshold=3, num_oov_buckets=1, vocab_filename='vocab')\n",
    "            \n",
    "            return {\n",
    "                'terms': terms,\n",
    "                'terms_indices': terms_indices,\n",
    "                'label': inputs['label']\n",
    "            }\n",
    "\n",
    "        \n",
    "        (transformed_train_data, transformed_metadata), transform_fn = (\n",
    "            (train_data, RAW_DATA_METADATA)\n",
    "            | 'AnalyzeAndTransform' >> beam_impl.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "\n",
    "        transformed_test_data, _ = (\n",
    "            ((test_data, RAW_DATA_METADATA), transform_fn)\n",
    "            | 'Transform' >> beam_impl.TransformDataset())\n",
    "        \n",
    "        transformed_data_coder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)\n",
    "\n",
    "        _ = (\n",
    "            transformed_train_data\n",
    "            | 'EncodeTrain' >> beam.Map(transformed_data_coder.encode)\n",
    "            | 'WriteTrain' >> tfrecordio.WriteToTFRecord('data/train_transformed.tfrecord'))\n",
    "\n",
    "        _ = (\n",
    "            transformed_test_data\n",
    "            | 'EncodeTest' >> beam.Map(transformed_data_coder.encode)\n",
    "            | 'WriteTest' >> tfrecordio.WriteToTFRecord('data/test_transformed.tfrecord'))\n",
    "        \n",
    "        _ = (\n",
    "            transform_fn\n",
    "            | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn('tft_output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['There are so many stupid moments in \\'Tower~of~Death\\'/\\'Game~of~Death~2\\' that you really wonder if it\\'s a spoof. At times, it felt like I was watching a sequel to Kung~Pow rather than a Bruce~Lee film.<br /><br />To be honest, this film has bugger all to do with \\'Game~of~Death\\'. If anything, it\\'s more a sequel/remake of \\'Enter~the~Dragon\\', incorporating many elements of that film - particularly the actual footage. Bruce~Lee\\'s character Billy~Lo (apparently) investigates the sudden death of his friend and encounters a piece of film that was left with the man\\'s daughter. When the body is stolen during the funeral (!), Billy is also killed and it\\'s up to his wayward brother to avenge both men\\'s deaths.<br /><br />Tong~Long stars as brother Bobby~Lo and doesn\\'t really have the sort of charisma to carry the film. His fighting abilities are very good however. Bruce~Lee obviously turns up thanks to (no longer) deleted footage simply to cash-in on the legacy. Saying that, on the whole, the footage is actually edited-in better than in \\'Game~of~Death\\' but it doesn\\'t stop the film from being a mess.<br /><br />OK, so the fights are actually very entertaining (dare I say mind-blowing) and make the film at least watchable. But there are so many daft elements to this film that it really tests your patience. First off, there\\'s the supposed villain who lives on his palatial estate... or is that mental institution? Seriously, the nutter eats raw venison, drinks deer\\'s blood, carries a monkey on his shoulder and owns some peacocks and lions (?!). This attempt to make him look tough and intelligent just makes you feel sorry for him - you half expect someone to escort him back to his room.<br /><br />In fact, this middle section is awful and when the scene involving a naked hooker and a lion suit arrived I turned it off. However, I did finish the film and was kind of glad I did because the fight scene towards the end (much like \\'GOD\\') was the whole reason for watching. While the story is an embarrassment, the action is very good and contains excellent choreography.<br /><br />But even the finale disappoints if the premise was anything to go by. What we were told was that the \\'Tower~of~Death\\' was a pagoda that was upside down and underground. This sounded great, like a twist on Bruce~Lee\\'s original idea with different styles of fighting on each level. Could this be the \\'Game~of~Death\\' that was originally planned? No! The film should have been named \"Generator~Room~of~Death\" because thats as far as the tower goes. Of yes, there were indeed one or two \\'different\\' styles... there were foil clad grunts, leopard-skinned henchman and stupid monk. It\\'s as though Enter~the~Dragon had never been made, with the plot being a poor imitation.<br /><br />Worth watching once for the fast paced fight scenes, but so stupid sometimes that it hurts. If this was intended, then fine. Thumbs up, however, for recreating that projector room scene from \\'Enter~The~Dragon\\'.'],\n",
       "       dtype=object),\n",
       " 'there|are|so|many|stupid|moments|in|tower~of~death|/|game~of~death~2|that|you|really|wonder|if|its|a|spoof|.|at|times|,|it|felt|like|i|was|watching|a|sequel|to|kung~pow|rather|than|a|bruce~lee|film|.|to|be|honest|,|this|film|has|bugger|all|to|do|with|game~of~death|.|if|anything|,|its|more|a|sequel|/|remake|of|enter~the~dragon|,|incorporating|many|elements|of|that|film|-|particularly|the|actual|footage|.|bruce~lees|character|billy~lo|(|apparently|)|investigates|the|sudden|death|of|his|friend|and|encounters|a|piece|of|film|that|was|left|with|the|mans|daughter|.|when|the|body|is|stolen|during|the|funeral|,|billy|is|also|killed|and|its|up|to|his|wayward|brother|to|avenge|both|mens|deaths|.|tong~long|stars|as|brother|bobby~lo|and|doesnt|really|have|the|sort|of|charisma|to|carry|the|film|.|his|fighting|abilities|are|very|good|however|.|bruce~lee|obviously|turns|up|thanks|to|(|no|longer|)|deleted|footage|simply|to|cash-in|on|the|legacy|.|saying|that|,|on|the|whole|,|the|footage|is|actually|edited-in|better|than|in|game~of~death|but|it|doesnt|stop|the|film|from|being|a|mess|.|ok|,|so|the|fights|are|actually|very|entertaining|(|dare|i|say|mind-blowing|)|and|make|the|film|at|least|watchable|.|but|there|are|so|many|daft|elements|to|this|film|that|it|really|tests|your|patience|.|first|off|,|theres|the|supposed|villain|who|lives|on|his|palatial|estate|.|or|is|that|mental|institution|?|seriously|,|the|nutter|eats|raw|venison|,|drinks|deers|blood|,|carries|a|monkey|on|his|shoulder|and|owns|some|peacocks|and|lions|.|this|attempt|to|make|him|look|tough|and|intelligent|just|makes|you|feel|sorry|for|him|-|you|half|expect|someone|to|escort|him|back|to|his|room|.|in|fact|,|this|middle|section|is|awful|and|when|the|scene|involving|a|naked|hooker|and|a|lion|suit|arrived|i|turned|it|off|.|however|,|i|did|finish|the|film|and|was|kind|of|glad|i|did|because|the|fight|scene|towards|the|end|(|much|like|god|)|was|the|whole|reason|for|watching|.|while|the|story|is|an|embarrassment|,|the|action|is|very|good|and|contains|excellent|choreography|.|but|even|the|finale|disappoints|if|the|premise|was|anything|to|go|by|.|what|we|were|told|was|that|the|tower~of~death|was|a|pagoda|that|was|upside|down|and|underground|.|this|sounded|great|,|like|a|twist|on|bruce~lees|original|idea|with|different|styles|of|fighting|on|each|level|.|could|this|be|the|game~of~death|that|was|originally|planned|?|no|!|the|film|should|have|been|named|generator~room~of~death|because|thats|as|far|as|the|tower|goes|.|of|yes|,|there|were|indeed|one|or|two|different|styles|.|there|were|foil|clad|grunts|,|leopard-skinned|henchman|and|stupid|monk|.|its|as|though|enter~the~dragon|had|never|been|made|,|with|the|plot|being|a|poor|imitation|.|worth|watching|once|for|the|fast|paced|fight|scenes|,|but|so|stupid|sometimes|that|it|hurts|.|if|this|was|intended|,|then|fine|.|thumbs|up|,|however|,|for|recreating|that|projector|room|scene|from|enter~the~dragon|.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qc the preprocessing\n",
    "with tf.Session() as sess:\n",
    "    qc1 = fetch_tf_records('data/train.tfrecord', RAW_DATA_FEATURE, top=10)\n",
    "    qc2 = fetch_tf_records('data/train_transformed.tfrecord*', transformed_metadata.schema.as_feature_spec(), top=10)\n",
    "    qc2 = sess.run(tf.sparse_to_dense(qc2['terms'].indices, qc2['terms'].dense_shape, qc2['terms'].values, default_value=''))\n",
    "\n",
    "qc1['review'][0], '|'.join(qc2[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize word embeddings with [GloVe](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary\n",
    "tf_transform_output = tft.TFTransformOutput('tft_output')\n",
    "vocab = tf_transform_output.vocabulary_by_name('vocab')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove embeddings\n",
    "embedding_size = 100\n",
    "glove_embeddings = {}\n",
    "\n",
    "with open('glove/glove.twitter.27B.{}d.txt'.format(embedding_size), mode='r') as f:  \n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        w = values[0]\n",
    "        vectors = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[w] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create initialized embedding matrix\n",
    "embedding_matrix = truncnorm.rvs(a=-2, b=2, size=(vocab_size+1, embedding_size))\n",
    "\n",
    "glove_np = pd.DataFrame(glove_embeddings).values\n",
    "glove_mu, glove_std = np.mean(glove_np), np.std(glove_np)\n",
    "        \n",
    "for i, w in enumerate(vocab):\n",
    "    try:\n",
    "        embedding_matrix[i] = np.clip((glove_embeddings[w] - glove_mu)/glove_std, -2, 2)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "embedding_matrix = embedding_matrix / math.sqrt(embedding_size)\n",
    "    \n",
    "def embedding_initializer(shape=None, dtype=tf.float32, partition_info=None):  \n",
    "    assert dtype is tf.float32\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input function\n",
    "feature_spec = tf_transform_output.transformed_feature_spec()\n",
    "\n",
    "def input_fn(input_file_pattern, num_epochs=None, batch_size=25, shuffle=True, prefetch=1):  \n",
    "    input_file_names = glob.glob(input_file_pattern)\n",
    "    \n",
    "    ds = tf.data.TFRecordDataset(input_file_names)\n",
    "    ds = ds.cache()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=1000, count=num_epochs))\n",
    "    else:\n",
    "        ds = ds.repeat(num_epochs)\n",
    "\n",
    "    ds = ds.apply(tf.contrib.data.map_and_batch(\n",
    "        map_func=lambda x: tf.parse_single_example(x, feature_spec), \n",
    "        batch_size=batch_size,\n",
    "        num_parallel_calls=multiprocessing.cpu_count()\n",
    "    ))\n",
    "    \n",
    "    if prefetch > 0:\n",
    "        ds = ds.prefetch(prefetch)\n",
    "    \n",
    "    features = ds.make_one_shot_iterator().get_next()\n",
    "    labels = features.pop('label')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create estimator spec\n",
    "def make_model(features, labels, mode, params, config):\n",
    "\n",
    "    # hyperparameters\n",
    "    dropout = params['dropout']\n",
    "    conv_filters = params['conv_filters']\n",
    "    dense_units = params['dense_units']\n",
    "    learning_start = params['learning_start']\n",
    "    \n",
    "    # set up feature columns\n",
    "    terms = features['terms_indices']\n",
    "    terms = tf.sparse_to_dense(terms.indices, terms.dense_shape, terms.values, default_value=vocab_size)    \n",
    "    terms_embed_seq = tf.contrib.layers.embed_sequence(terms, vocab_size=vocab_size+1, embed_dim=embedding_size, initializer=embedding_initializer)\n",
    "    \n",
    "    # build graph\n",
    "    net = terms_embed_seq\n",
    "    net = tf.layers.dropout(net, rate=dropout, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "    net = tf.layers.conv1d(inputs=net, filters=conv_filters, kernel_size=3, strides=1, activation=tf.nn.leaky_relu)\n",
    "    net = tf.reduce_max(input_tensor=net, axis=1)      \n",
    "    net = tf.layers.dense(net, units=dense_units, activation=tf.nn.leaky_relu)\n",
    "    net = tf.layers.dropout(net, rate=dropout, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "    logits = tf.layers.dense(net, 2)\n",
    "    \n",
    "    # compute predictions\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    predicted_probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    # generate predictions\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class': predicted_classes,\n",
    "            'prob': predicted_probs\n",
    "        }\n",
    "        \n",
    "        export_outputs = {\n",
    "          'predict': tf.estimator.export.PredictOutput(outputs=predictions)\n",
    "        }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "\n",
    "    # compute loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # create training op with cosine annealing for learning rate\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        \n",
    "        learning_rate = tf.train.cosine_decay(learning_rate=learning_start, global_step=global_step, alpha=0.05, decay_steps=5000)\n",
    "        \n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
    "        \n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # compute evaluation metrics\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predicted_classes),\n",
    "        'auc': tf.metrics.auc(labels=labels, predictions=predicted_probs[:, 1])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifiers for each set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make hyperparameter grid\n",
    "param_grid = {\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'conv_filters': [100, 200, 300],\n",
    "    'dense_units': [100, 200, 300],\n",
    "    'learning_start': [0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "param_grid = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 13min 26s, sys: 5min 19s, total: 1h 18min 46s\n",
      "Wall time: 51min 14s\n"
     ]
    }
   ],
   "source": [
    "# train all our models\n",
    "classifiers = []\n",
    "classifiers_stats = []\n",
    "\n",
    "def train():\n",
    "    for p in param_grid:\n",
    "        classifier = tf.estimator.Estimator(model_fn=make_model, params=p,\n",
    "                                            config=tf.estimator.RunConfig(session_config=tf.ConfigProto(log_device_placement=True)))\n",
    "\n",
    "        classifier.train(input_fn=lambda: input_fn('data/train_transformed.tfrecord*', num_epochs=5))\n",
    "        classifier_stats = classifier.evaluate(input_fn=lambda: input_fn('data/test_transformed.tfrecord*', num_epochs=1))\n",
    "\n",
    "        classifiers.append(classifier)\n",
    "        classifiers_stats.append(classifier_stats)\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and select best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_filters': 300,\n",
       " 'dense_units': 200,\n",
       " 'dropout': 0.3,\n",
       " 'learning_start': 0.1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select best hyperparameters\n",
    "results = zip(param_grid, classifiers, classifiers_stats)\n",
    "results.sort(key=lambda x: -x[2]['auc'])\n",
    "best_params, best_classifier, best_classifier_stats = results[0]\n",
    "del results\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.115512</td>\n",
       "      <td>0.239472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.991893</td>\n",
       "      <td>0.967264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_step</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.960680</td>\n",
       "      <td>0.905600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   train         test\n",
       "loss            0.115512     0.239472\n",
       "auc             0.991893     0.967264\n",
       "global_step  5000.000000  5000.000000\n",
       "accuracy        0.960680     0.905600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall stats\n",
    "train_stats = best_classifier.evaluate(input_fn=lambda: input_fn('data/train_transformed.tfrecord*', num_epochs=1))\n",
    "test_stats = best_classifier.evaluate(input_fn=lambda: input_fn('data/test_transformed.tfrecord*', num_epochs=1))\n",
    "\n",
    "train_stats = pd.DataFrame.from_dict(train_stats, orient='index', columns=['train'])\n",
    "test_stats = pd.DataFrame.from_dict(test_stats, orient='index', columns=['test'])\n",
    "stats = train_stats.join(test_stats)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    review = tf.placeholder(dtype=tf.string)\n",
    "    label = tf.zeros(dtype=tf.int64, shape=[1, 1]) # just a placeholder\n",
    "    \n",
    "    transformed_features = tf_transform_output.transform_raw_features({'review': review, 'label': label})\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(transformed_features, {'review': review})\n",
    "\n",
    "\n",
    "export_path = best_classifier.export_savedmodel(export_dir_base='exports',\n",
    "                                                serving_input_receiver_fn=serving_input_fn)\n",
    "\n",
    "export_path = export_path.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-09-30 04:46:06.528179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
       " '2018-09-30 04:46:06.528428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: ',\n",
       " 'name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285',\n",
       " 'pciBusID: 0000:00:04.0',\n",
       " 'totalMemory: 15.90GiB freeMemory: 356.06MiB',\n",
       " '2018-09-30 04:46:06.528456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0',\n",
       " '2018-09-30 04:46:06.900881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:',\n",
       " '2018-09-30 04:46:06.900935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 ',\n",
       " '2018-09-30 04:46:06.900946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N ',\n",
       " '2018-09-30 04:46:06.901104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 65 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)',\n",
       " '2018-09-30 04:46:08.204809: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 66.70MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.',\n",
       " 'Result for output key class:',\n",
       " '[0 1]',\n",
       " 'Result for output key prob:',\n",
       " '[[0.9057794  0.09422061]',\n",
       " ' [0.00852456 0.99147546]]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!venv/bin/saved_model_cli run --input_exprs 'review=[\"this is a terrible movie\", \"this is a great movie\"]'  \\\n",
    "--dir $export_path --tag_set serve --signature_def predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie-reviews",
   "language": "python",
   "name": "movie-reviews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
